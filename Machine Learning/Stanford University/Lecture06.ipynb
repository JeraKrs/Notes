{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture06 Regularization\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "**Overfitting**: If there are too many features, the learned hypothesis may fit the training set very well ($g(\\theta) \\approx 0$), but fail to generalize to new examples.\n",
    "\n",
    "There are two methods for addressing overfitting:\n",
    "1. Reduce the number of features:\n",
    "    * manually select which features to keep.\n",
    "    * use a model selection algorithm.\n",
    "2. Regularization\n",
    "    * keep all the features, but reduce the magnitude of parameters $\\theta_j$.\n",
    "    * regularization works well when we have a lot of slightly useful features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The regularization modifies the cost function to regularize all of theta parameters in a single summation:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum^{n}_{j=1}\\theta_j^2$$\n",
    "\n",
    "where the $\\lambda$ is called as **regularization parameter**. It determines how much the costs of the theta parameters are inflated.\n",
    "\n",
    "Using the above cost function with the extra summation, it can smooth the output of the hypothesis function to reduce overfitting. But\n",
    "* if lambda is chosen to be too large, it may smooth out the function too much and cause underfitting;\n",
    "* if lambda is chosen to be too small, it may not reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "**Gradient Descent**:\n",
    "\n",
    "Repeat {\n",
    "\n",
    "$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}(h_{\\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}$\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\left [ \\left ( \\frac{1}{m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\theta_j  \\right ]$    $\\space\\space\\space j \\in \\{1, \\dots, n\\}$\n",
    "\n",
    "}\n",
    "\n",
    "**Normal Equation**: $\\theta = (X^TX  + \\lambda \\cdot L)X^Ty$ where $L = \\begin{bmatrix} 0 &  &  &  & \\\\  &  1 &  &  & \\\\  &  &  1 &  & \\\\  &  &  & \\ddots & \\\\  &  &  &  & 1 \\end{bmatrix} $.\n",
    "\n",
    "Recall that if $m < n$, then $X^TX$ is non-invertible, but if it adds $\\lambda L$, it becomes invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularized Logistic Regression\n",
    "\n",
    "The cost function: $J(\\theta) = \\frac{1}{m}\\sum^m_{i=1} \\left [  -y^{(i)}\\log(h_{\\theta}(x^{(i)})) - (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)})) \\right ] + \\frac{\\lambda}{2m}\\sum^n_{j=1}\\theta_j^2$ where the second sum, $\\sum^n_{j=1}\\theta_j^2$ means to explicitly exclude the bias term, $\\theta_0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
