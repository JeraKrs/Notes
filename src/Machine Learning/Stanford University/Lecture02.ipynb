{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture02 Linear Regression with One Variable\n",
    "\n",
    "\n",
    "## Model Representation\n",
    "\n",
    "* $x^{(i)}$ denotes the “input” variables, also called input features.\n",
    "* $y^{(i)}$ denotes the “output” or target variable that the models are trying to predict.\n",
    "* A pair $(x^{(i)}, y^{(i)})$ is called a training example.\n",
    "* A list of $m$ training examples $(x(i),y(i))$ where $i=1,2,...$, m-is called a training set.\n",
    "\n",
    "For solving a supervised learning problem, the goal is, given a training set, to learn a function $h: X \\rightarrow Y$ so that $h(x)$ is a \"good\" predictor for the corresponding value of $y$, this function $h$ is called a **hypothesis**.\n",
    "\n",
    "![Model Representation](./images/figure02-01.png)\n",
    "\n",
    "* Regression problem: when the predict variable is continuous.\n",
    "* Classification problem: when the predict variable is discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The **cost function** uses to measure the accuracy of a hypothesis function. The function $J_{(\\theta_0, \\theta_1)}$ denotes the difference between the predicted value and the actual value:\n",
    "\n",
    "$$J_{(\\theta_0, \\theta_1)} = \\frac{1}{2m} \\sum^{m}_{i=1} (\\hat{y_i} - y_i)^2 = \\frac{1}{2m} \\sum^{m}_{i=1} (h_{\\theta}(x_i) - y_i)^2$$\n",
    "\n",
    "where the function is otherwise called the \"Squared error function\", or \"Mean squared error\", the mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent.\n",
    "\n",
    "The objective is to find the suitable parameter $\\theta$ which can minimize the cost function for the given data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parameter Learning\n",
    "\n",
    "**Problem description**:\n",
    "* Have some function $J(\\theta_0, \\theta_1)$.\n",
    "* Want $\\min_{\\theta_0, \\theta_1} J(\\theta_0, \\theta_1)$.\n",
    "\n",
    "**Outline**:\n",
    "* Start with some $\\theta_0, \\theta_1$.\n",
    "* Keep changing $\\theta_0, \\theta_1$ to reduce $J(\\theta_0, \\theta_1)$.\n",
    "\n",
    "**Gradient Descent Algorithm**\n",
    "\n",
    "repeat until convergence {  $\\theta_j := \\theta_j + \\alpha \\frac{\\partial  }{\\partial   \\theta_j} J(\\theta_0, \\theta_1)$ }. For all $\\theta_j$ where $j=1,\\dots,n$ should be simultaneously updated.\n",
    "\n",
    "$\\alpha$ is the learning rate:\n",
    "* if $\\alpha$ is too small, gradient descent can be slow.\n",
    "* if $\\alpha$ is too large, gradient descent can overshoot the minimum, it may fail to converge or even diverge.\n",
    "\n",
    "Different start status will lead to different result which may be a local minimum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
