{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture08 Neural Networks: Learning\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "* $L$ = total number of layers in the network.\n",
    "* $s_l$ = number of units (not counting bias unit) in layer $l$.\n",
    "* $K$ = number of output units/classes.\n",
    "\n",
    "The cost function for neural networks: \n",
    "\n",
    "$$\\begin{gather*} J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2\\end{gather*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "\"Backpropagation\" is neural-network terminology for minimizing the cost function, which means the goal is $min_{\\Theta}J(\\Theta)$.\n",
    "\n",
    "The processes of backpropagation for given training set $\\left \\{ (x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)}) \\right \\}$:\n",
    "    \n",
    "1. $\\Delta^{(l)}_{i,j} := 0$ for all $(l,i,j)$.\n",
    "2. For training example $t =1$ to $m$:\n",
    "    * Set $a^{(1)} := x^{(t)}$.\n",
    "    * Perform forward propagation to compute $a^{(l)}$ for $1, \\dots, l$.\n",
    "3. Using $y^{(t)}$, compute $\\delta^{(L)} = a^{(L)} - y^{(t)}$.\n",
    "4. Compute $\\delta^{(L-1)}, \\delta^{(L-2)},\\dots,\\delta^{(2)}$ using $\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)})$.\n",
    "5. $\\Delta_{i,j}^{(l)} := \\Delta_{i,j}^{(l)} + a_{j}^{(l)}\\delta_i^{(l+1)}$ or with vectorization $\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T$.\n",
    "\n",
    "Define the $D_{i,j}^{(l)} := \\frac{\\partial }{ \\partial  \\Theta^{(l)}_{i,j}}J(\\Theta)$, where $D_{i,j}^{(l)} = \\left\\{\\begin{matrix} \\frac{1}{m} (\\Delta^{(l)}_{i,j} + \\lambda\\Theta^{(l)}_{i,j}) & if j \\neq 0 \\\\ \\frac{1}{m} \\Delta^{(l)}_{i,j} &  if j = 0 \\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementation Note: Unrolling Parameters\n",
    "\n",
    "In order to use optimizing functions such as \"fminunc()\", it can \"unroll\" all the elements and put them into one long vector:\n",
    "\n",
    "``` Octave\n",
    "thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]\n",
    "deltaVector = [ D1(:); D2(:); D3(:) ]\n",
    "```\n",
    "\n",
    "it can get back the original matrices from the \"unrolled\" versions as follows:\n",
    "\n",
    "``` Octave\n",
    "Theta1 = reshape(thetaVector(1:110),10,11)\n",
    "Theta2 = reshape(thetaVector(111:220),10,11)\n",
    "Theta3 = reshape(thetaVector(221:231),1,11)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "Gradient checking will assure that otheur backpropagation works as intended, it can approximate the derivative of the cost function with:\n",
    "\n",
    "$$\\frac{\\partial }{\\partial \\Theta}J(\\Theta) = \\frac{J(\\Theta + \\epsilon) - J(\\Theta - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "Once we have verified once that our backpropagation algorithm is correct, it doesn't need to compute gradApprox again. The code to compute gradApprox can be very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization\n",
    "\n",
    "Initializing all theta weights to zero does not work with neural networks, so it needs to use the random initialization for breaking symmetry. While the random initialization will initialize each $\\Theta^{(l)}_{i,j}$ to a random value in $[-\\epsilon, \\epsilon]$.\n",
    "\n",
    "``` Octave\n",
    "% If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.\n",
    "% rand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1.\n",
    "\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it Together\n",
    "\n",
    "**Pick a network architecture**:\n",
    "* Number of input units = dimension of features $x^{(i)}$.\n",
    "* Number of output units = number of classes.\n",
    "* Reasonable default: 1 hidden layer or if >1 hidden layer, have same numbefr of hidden units in every layer.\n",
    "\n",
    "**Training a neural network**:\n",
    "* randomly initial weights.\n",
    "* implement forward propagation to get $h_{\\Theta}(x^{(i)})$ for any $x^{(i)}$.\n",
    "* implement code to comput cost function $J(\\Theta)$.\n",
    "* implement backprop to comput partial derivatices $\\frac{\\partial }{\\partial  \\Theta_{j,k}^{(l)}}J(\\Theta)$.\n",
    "* using gradient checking to confirm that the backpropagation works, then disable gradient checking.\n",
    "* usnig gradient descent or a built-in optimization function to minimize the cost function with the weights in theta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
