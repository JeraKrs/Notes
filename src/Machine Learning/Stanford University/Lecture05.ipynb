{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture05 Logistic Regression\n",
    "\n",
    "## Classification and Representation\n",
    "\n",
    "To attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn't work well because classification is not actually a linear function.\n",
    "\n",
    "**Logistic Regression Model**: $h_{\\theta}(x) = g(\\theta^Tx) = g(z) = \\frac{1}{1+e^{-z}}$, where $g(z)$ is denoted as logistic function or sigmoid function.\n",
    "* interpretation of hypothesis output: $h_{\\theta}(x)$ = estimated probability that y=1 on input x.\n",
    "* decision boundary: $y = 1 \\Rightarrow h_{\\theta}(x) \\geq 0.5 \\Rightarrow g(z) \\geq 0.5 \\Rightarrow \\theta^Tx \\geq 0.5$. Therefore, the descision boundary is the line that separates the area where y=0 and where y=1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "\n",
    "Training Set: $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \\dots, (x^{(m)}, y^{(m)})$, m examples and x is a $(n+1) \\times 1$ vector while $x_0 = 1$, $y \\in \\{0, 1\\}$.\n",
    "\n",
    "Since the $J(\\theta) = Cost(h_{\\theta}(x), y) = \\frac{1}{2}(h_{\\theta}(x) - y)^2$ is non-convex, the logistic regression model uses $Cost(h_{\\theta}(x), y) = \\left\\{\\begin{matrix} -log(h_{\\theta}(x)) & if & y = 1 \\\\ -log(1 - h_{\\theta}(x)) & if & y = 0 \\end{matrix}\\right.$ as the **cost function**.\n",
    "* If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.\n",
    "* If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.\n",
    "\n",
    "**Simplified cost function**: $Cost(h_{\\theta}(x), y) = -y log(h_{\\theta}(x)) - (1 - y) log(1 - h_{\\theta}(x))$.\n",
    "\n",
    "The $J(\\theta) = \\frac{1}{m}\\sum^m_{i=1}Cost(h_{\\theta}(x), y)$:\n",
    "* to fit parameter $\\theta$: $min_{\\theta} J(\\theta)$.\n",
    "* to make prediction for given new x, output $h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^tx}}$.\n",
    "\n",
    "\n",
    "**Gradient descent**: algorithm looks identical to linear regression.\n",
    "\n",
    "**Optimization algorithm**: Conjugate gradient, BFGS, L-BFGS. They have some advantages which is no need to manually pick $\\alpha$ and oftern faster than gradient descent, but they are more complex.\n",
    "\n",
    "To use the library of the optimization algorithms, it needs to write a single function that returns both of these:\n",
    "\n",
    "``` Octave\n",
    "function [jVal, gradient] = costFunction(theta)\n",
    "  jVal = [...code to compute J(theta)...];\n",
    "  gradient = [...code to compute derivative of J(theta)...];\n",
    "end\n",
    "```\n",
    "\n",
    "Then we can use octave's \"fminunc()\" optimization algorithm along with the \"optimset()\" function that creates an object containing the options we want to send to \"fminunc()\". \n",
    "\n",
    "``` Octave\n",
    "options = optimset('GradObj', 'on', 'MaxIter', 100);\n",
    "initialTheta = zeros(2,1);\n",
    "   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification: one-vs-all\n",
    "\n",
    "Train a logistic regression classifier $h_{\\theta}^i(x)$ for each class $i$ to predict the probability that $y=i$.\n",
    "\n",
    "To make a prediction on a new $x$, pick the class $i$ that maximizes the $h_{\\theta}(x)$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
